# cov-analyze-metrics
A Coverity extension to support code metrics defects.

# Metrics Defects


During the analysis of the code base, Coverity generates a file (`FUNCTION.metrics.xml.gz`) containing a number 
of code metrics for each of the functions emitted. This file is located into each of the output directories 
`<idir>/output`

From the Coverity portal (Connect) these metrics appear under the view "Functions". It is possible to filter 
those functions that have  a number above a threshold like the LOC, cyclomatic complexity, etc.

The purpose of **cov-analyze-metrics** is to extract these metrics from the intermediate directory, process them 
against a set of customized checkers and when a violation of the threshold(s) defined for a checker then
trigger a new defect in a JSON file.
 
This command is for creating custom defects to be imported in the same intermediate directory 
using the regular cov-import-results command from Coverity. Then, defects from the regular analysis 
and those generated from metrics violation with cov-analyze-metrics need to be committed to be
available to Coverity Connect users.

# Using cov-analyze-metrics

## Running the command

The Java version is more recent one and provide more flexibility, features and performance. Everything
is embedded in the JAR file in the resource folder (like all default checkers with their configuration)

There's no specific installation to be done, just run the command usig a JRE 8+

````
java -jar cov-analyze-metrics-1.0.jar --help
````

## Configuring checkers

Enabling a checker: `--all` or `--enable-checker CHECKER_NAME`

Configuring a checker: `--checker-option CHECKER:OPTION:VALUE`

## Adding new checkers

Create a specific folder with all the JSON file describing your new checkers.

Each new checker is defined in one single JSON file.

When launching the analyzer, add the option `--config-dir <dir>` for the analyzer to load the
new checker defined there.
 

## Step by Step ...

To generate defects from your metric checkers with cov-analyze-metrics proceed as follow:

* Perform the cov-build / cov-analyze as usual. 
=> This should make available the function metrics file in the `<idir>/output` directory
* Invoke the cov-analyze-metrics tool pointing to your intermediate directory and output tag if applicable.
=> This should generate a JSON file with the custom code metrics defects 
* Invoke the cov-import-results on the custom defect JSON
=> This will add the Code Metrics defects to the idir
* Invoke the usual commit command to push the defects (those generated by Coverity and those added by cov-analyze-metrics)

```
cov-build --dir idir ../..
cov-analyse --dir idir --output-tag -quality --all ../..
cov-analyze-metrics  --dir idir --output-tag -quality --all -co METRICS.LOC_TOO_HIGH:loc:260
```

# Code Metrics Checkers

## METRICS.LOC_TOO_HIGH

Creates defect on function with a number of line of code above a threshold. You can set the
value of the threshold with `-co METRICS.LOC_TOO_HIGH:loc:<value>`

## METRICS.CC_TOO_HIGH

Creates defect on function with a cyclomatic complexity above a threshold. You can set the
value of the threshold with `-co METRICS.CC_TOO_HIGH:cc:<value>`
 
## METRICS.TOO_COMPLEX

This is a combination of `METRICS.LOC_TOO_HIGH` and `METRICS.CC_TOO_HIGH`. Both thresholds are 
available and the defect is created if both conditions are verified (logical AND).

## METRICS.FILE_TOO_LONG

Creates defect on file with total number of line of code, that is the sum if the LOC of each
function implemented in the file, above a threshold. You can set the
value of the threshold with `-co METRICS.FILE_TOO_LONG:cc:<value>`

Todo: Implement this checker.

## METRICS.HIGHER_LOC

Creates defect on function that have a line of code count in the highest percentile for the 
whole code being analyzed. The checker can be configured with the percentile. For example,
setting the percentile to 10 means that only the functions with a LOC in the 10% of the
largest ones will have the defect. This checker also support a second threshold `loc` with 
the minimum number of LOC that will trigger a defect.

Thus setting the percentile to 10 and the minimum to 150 means that only the functions with
at least 150 LOC and with the LOC in the higher 10% will have the defect.

Todo: Implement this checker.

# For the Future

## Enhancements

* Todo: Allow each checker to define the list of file regex patterns with functions that should not be analyzed
* Todo: (done) Allow to exclude some file from the analysis of any of the selected checkers.
* Todo: Allow a checker to define a threshold based on global statistic such as mean, percentile, ...
* Todo: Allow a checker to apply to the metrics of all the functions of a single file (like sum of LoC, Max of CC)
* Todo: Embed the default configuration files (checkers, etc.) into the resources of the Jar file
* Todo: Add support for a per Checker cutoff or global cutoff and keep only the most significant defects in the cut
* Todo: Add support to compare metrics in local analysis with metrics already committed


## Bug fixes

* Fixme Configure the Log4J from the command line