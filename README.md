# cov-analyze-metrics
A Coverity extension to support code metrics defects.

# Metrics Defects


During the analysis of the code base, Coverity generates a file (`FUNCTION.metrics.xml.gz`) containing a number 
of code metrics for each of the functions emitted. This file is located into each of the output directories 
`<idir>/output`

From the Coverity portal (Connect) these metrics appear under the view "Functions". It is possible to filter 
those functions that have  a number above a threshold like the LOC, cyclomatic complexity, etc.

The purpose of **cov-analyze-metrics** is to extract these metrics from the intermediate directory, process them 
against a set of customized checkers and when a violation of the threshold defined for one of the checker then 
trigger a new defect in a JSON file to be imported in the same intermediate directory.


# Using cov-analyze-metrics

## Installing cov-analyze-metrics
Extract the archive somewhere. Make sure it contains:

* `<install>/bin`
This folder contains the main script and the XSLT processing script to convert XML into CVS
* `<install>/config`
This folder contains the METRICS_common.sh script imported by each checker script and also the checker scripts themselves. Make sure that the checker script are named after the checker's name: checker FOO has a <install>/config/FOO.sh script.

## Command line options

To obtain the command line option, run 
```
cov-analyze-metrics.sh --help
```
## Configuring Code Metrics Checkers

From the command line, you can activate and change the threshold of a checker using the option 
`--add-threshold CHECKER_NAME=<threhsold_value>`

For example:
```
cov-analyze-metrics.sh ....  --add-threshold METRICS.LOC_TOO_HIGH=267 ...
```
## Running cov-analyze-metrics

To use this tools you should proceed as follow:

* Perform the cov-build / cov-analyze as usual. 
=> This should make available the function metrics file in the `<idir>/output` directory
* Invoke the cov-analyze-metrics tool pointing to your intermediate directory and output tag if applicable.
=> This should generate a JSON file with the custom code metrics defects 
* Invoke the cov-import-results on the custom defect JSON
=> This will add the Code Metrics defects to the idir
* Invoke the usual commit command to push the defects (those generated by Coverity and those added by cov-analyze-metrics)

```
cov-build --dir idir ../..
cov-analyse --dir idir --output-tag -quality --all ../..
cov-analyze-metrics.sh  --dir idir --output-tag -quality --enable-default --add-threshold METRICS.LOC_TOO_HIGH=260
```

# How it works ?
The current implementation of cov-analyze-metrics is based on Linux Bash shells and a couple of dependent tools:

* xsltproc - Used to parse the function metrics file generated by Coverity.
* sed - Used to clean-up the processed data from funtion metrics files
* jq - To format and check the JSON file containing the generated defects
* others: zcat, ...

## Principles
I will not expand too much in the internal of the tool since the current version is subject to a complete 
re-engineering to make it more flexible for creating and configuring new checker and to optimize processing time.

The basic idea is that the main script will extract from the FUNCTION.metrics.xml.gz the metrics data for each 
function and process them another scripts, one for each checker. There's a checker script for each checker 
(for example there's `<install dir>/config/METRICS.LOC_TOO_HIGH.sh` for the checker `METRICS.LOC_TOO_HIGH`), 
which may use a common script `<install dir>/config/METRICS_common.sh`

## Processing stages

### Parameters verification:

* The intermediate directory exists at the location specified by --dir option
* The output directory in the intermediate directory exists at the location specified by --output-tag
* The function metrics file exists in the intermediate directory specified by the --dir option and the --output-tag option.
* Creates a temp folder named tmpin the installation directory of the tool
* There's at least one checker activated on the command line (option --enable-default or --add-threshold)
If one of those verification failed the execution of script is aborted.

### Extract function metrics
There's XSLT processing template to extract the metrics of each function and produce a CSV file named nnn-rawmetric.csv (where nnn is the PID of the shell process) in the temp directory
The resulting CSV file is made of
* The pathname of the file
* The name of the function
* A sequence of metrics <metric_name>:<metric_value>

### Process each function / checker
* For each of the line in the extracted metrics:
+ For each enabled checker
- Apply the checker script to be found in the configuration directory after the name of the checker. The current metrics line and the checker threshold are provided to the script as command line option.
- If the checker's script find a defect it returns on the console the corresponding JSON excerpt in the cov-import-result format
- Add the output of the checker script to the whole cov-import-result JSON file

### Perform import of metrics defect JSON file
